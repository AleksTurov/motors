{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec69ad21",
   "metadata": {},
   "source": [
    "# Motor Remaining Useful Life (RUL) Prediction: Baseline Pipeline\n",
    "\n",
    "This notebook demonstrates a baseline approach for predicting the Remaining Useful Life (RUL) of motors using a dataset similar to NASA C-MAPSS. The workflow includes data loading, feature engineering, model training (XGBoost), and evaluation. \n",
    "\n",
    "---\n",
    "\n",
    "**Outline:**\n",
    "1. Import libraries and load data\n",
    "2. Initial data analysis and visualization\n",
    "3. Add RUL target variable\n",
    "4. Aggregate features with rolling window\n",
    "5. Feature engineering: derivatives, rolling stats, PCA\n",
    "6. Train/test split\n",
    "7. Baseline XGBoost model\n",
    "8. Model evaluation and error analysis\n",
    "9. (Optional) Data preparation for LSTM/GRU (sliding window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import catboost\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/Data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e211ffe",
   "metadata": {},
   "source": [
    "## 1. Initial Data Analysis and Visualization\n",
    "\n",
    "- Check data structure, types, and missing values\n",
    "- Visualize cycle and sensor distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataframe info and check for missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b036a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic statistics. Sensors p01 and p00, p07, p09, p10, p16 and p17 are not changed\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e52f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns[df.nunique() < 2]) # Identify columns with only one unique value\n",
    "\n",
    "# Drop columns with only one unique value\n",
    "df = df.loc[:, df.nunique() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681afd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of cycles per motor\n",
    "plt.figure(figsize=(8,4))\n",
    "df.groupby('id')['cycle'].max().hist(bins=50)\n",
    "plt.title('Distribution of Maximum Cycles per Motor')\n",
    "plt.xlabel('Max Cycle')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019dc20f",
   "metadata": {},
   "source": [
    "## 2. Add RUL (Remaining Useful Life) Target Variable\n",
    "\n",
    "- For each motor, calculate RUL as the difference between the maximum cycle and the current cycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RUL for each row \n",
    "df_target = df.groupby('id')['cycle'].transform('max')\n",
    "\n",
    "df['max_cycle'] = df_target.copy()\n",
    "\n",
    "df['rul'] = df['max_cycle'] - df['cycle']\n",
    "df.drop('max_cycle', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECDF plot for RUL (Remaining Useful Life)\n",
    "#This plot shows the ECDF (Empirical Cumulative Distribution Function) of RUL. \n",
    "# It helps us see the distribution of remaining useful life for all engines.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.displot(data=df, x='rul', kind='ecdf')\n",
    "plt.title('ECDF of RUL')\n",
    "plt.xlabel('RUL (cycles)')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation with RUL. There are p02, p15, p03, p18, p06, p13 that are highly correlated with RUL.\n",
    "# Cycle type features also strongly correlate with RUL (as expected, since RUL = max_cycle - cycle).\n",
    "print(df.corr().abs()['rul'].sort_values(ascending=False))\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column like as the cycle is time series data, so we need to create rolling features\n",
    "\n",
    "# Note: When you create rolling window features, the first (window_size-1) rows for each engine will have NaN values.\n",
    "# This is normal because there is not enough history for the window.\n",
    "# To use PCA or any model that does not support NaN, you should fill these NaN values.\n",
    "# The most common way is to fill NaN with the mean or median of the column.\n",
    "\n",
    "sensor_cols = ['p02', 'p03', 'p04', 'p05', 'p06', 'p08', 'p11', 'p12',\n",
    "       'p13', 'p14', 'p15', 'p17', 'p18', 'p19', 'p20', 's1', 's2']\n",
    "sensor_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63a049",
   "metadata": {},
   "source": [
    "## 3. Aggregate Features with Rolling Window\n",
    "\n",
    "- For each motor and each cycle, compute rolling window aggregates (mean, std, min, max) for sensor \n",
    "- This helps capture recent trends and variability for each engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51491ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column like as the cycle is time series data, so we need to create rolling features\n",
    "\n",
    "# Note: When you create rolling window features, the first (window_size-1) rows for each engine will have NaN values.\n",
    "# This is normal because there is not enough history for the window.\n",
    "# To use PCA or any model that does not support NaN, you should fill these NaN values.\n",
    "# The most common way is to fill NaN with the mean or median of the column.\n",
    "\n",
    "# Add difference features for each sensor column\n",
    "for col in sensor_cols:\n",
    "    df[f'{col}_diff1'] = df.groupby('id')[col].diff()\n",
    "\n",
    "def add_rolling_features(df, cols, window):\n",
    "    \"\"\" \n",
    "    Function to add rolling features for each column in cols\n",
    "    using a specified window size.\n",
    "    \"\"\"\n",
    "    new_features = {}\n",
    "    for func in ['mean', 'std', 'min', 'max']:\n",
    "        for col in sensor_cols:\n",
    "            new_features[f'{col}_roll{window}_{func}'] = (\n",
    "                df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).agg(func))\n",
    "            )\n",
    "    df = pd.concat([df, pd.DataFrame(new_features)], axis=1)\n",
    "    df = df.copy()  # Defragment the DataFrame\n",
    "    # Fill NaN values with the mean of the column\n",
    "    for col in new_features.keys():\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    return df\n",
    "\n",
    "windows = [5, 10, 20, 30, 45, 60, 90, 120, 180, 350]\n",
    "# Add rolling features for each window size\n",
    "for window in windows:\n",
    "    df = add_rolling_features(df, sensor_cols, window)\n",
    "\n",
    "df[[c for c in df.columns if 'roll' in c]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_trend(df, cols, window):\n",
    "    \"\"\"\n",
    "    Function to add rolling trend features for each column in cols\n",
    "    using a specified window size.\n",
    "    The trend is calculated using a linear regression fit (slope).\n",
    "    \n",
    "    \"\"\"\n",
    "    def trend(x):\n",
    "        idx = np.arange(len(x))\n",
    "        if len(x) < 2:\n",
    "            return 0.0\n",
    "        return np.polyfit(idx, x, 1)[0]\n",
    "    new_features = {}\n",
    "    for col in cols:\n",
    "        new_features[f'{col}_roll{window}_trend'] = (\n",
    "            df.groupby('id')[col]\n",
    "              .transform(lambda x: x.rolling(window, min_periods=2).apply(trend, raw=True))\n",
    "        )\n",
    "    df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)\n",
    "    df = df.copy()  \n",
    "    return df\n",
    "\n",
    "\n",
    "for w in windows:\n",
    "    df = add_rolling_trend(df, sensor_cols, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7a6c6",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Derivatives, Rolling Statistics, PCA\n",
    "\n",
    "- Add first-order differences (derivatives) for sensor features.\n",
    "- Optionally, apply PCA to reduce dimensionality of rolling features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "rolling_cols = [c for c in df.columns if 'roll' in c or 'diff1' in c]\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[rolling_cols] = imputer.fit_transform(df[rolling_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73592ccc",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split\n",
    "\n",
    "- Split the data into training and test sets. We will use the last 20 cycles of each engine as the test set. The rest will be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fa8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for RUL to stratify the split\n",
    "# This helps to ensure that both training and test sets have a similar distribution of RUL values\n",
    "# We use pd.qcut to create quantile-based bins, which helps in stratifying the split\n",
    "# The 'duplicates' parameter is set to 'drop' to avoid issues with bins that have the same edges\n",
    "# This is useful when the RUL values are not evenly distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for RUL to stratify the split\n",
    "id_rul = df.groupby('id')['rul'].max().reset_index()\n",
    "id_rul['rul_bin'] = pd.qcut(id_rul['rul'], q=10, duplicates='drop')\n",
    "id_rul['rul_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets, stratifying by the RUL bins\n",
    "train_ids, test_ids = train_test_split(\n",
    "    id_rul['id'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=id_rul['rul_bin']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form train and test sets based on the selected ids\n",
    "train_df = df[df['id'].isin(train_ids)].copy()\n",
    "test_df = df[df['id'].isin(test_ids)].copy()\n",
    "\n",
    "print(train_df['rul'].describe())\n",
    "print(test_df['rul'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of RUL in train and test sets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(train_df['rul'], bins=30, alpha=0.5, label='train')\n",
    "plt.hist(test_df['rul'], bins=30, alpha=0.5, label='test')\n",
    "plt.xlabel('RUL')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.title('Train/Test RUL Distribution (no id overlap)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "stat, p_value = ks_2samp(train_df['rul'], test_df['rul'])\n",
    "print(f\"KS statistic: {stat:.4f}, p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value > 0.05:\n",
    "    print(\"rul distributions in train and test are statistically similar.\")\n",
    "else:\n",
    "    print(\"rul distributions in train and test are statistically different.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf47d2d",
   "metadata": {},
   "source": [
    "## 6. Baseline XGBoost Model\n",
    "\n",
    "- We will train a simple XGBoost model to predict RUL. We use only the rolling window features, derivatives, and PCA features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in train_df.columns if col not in ['rul', 'id', 'cycle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['rul']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['rul']\n",
    "\n",
    "# XGBoost\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=1000, max_depth=5, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "print(\"XGBoost:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted RUL: {y_pred_xgb[i]:.1f}, True RUL: {y_test.iloc[i]}\")\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    # Main metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"MAE (all): {mae:.2f}\")\n",
    "    print(f\"RMSE (all): {rmse:.2f}\")\n",
    "\n",
    "    # there are some engines with RUL > 200, so we need to analyze errors by ranges\n",
    "    bins = [0, 60, 100, 200, np.inf]\n",
    "    labels = ['<=60','60-100','100-200', '>200']\n",
    "    y_test_bins = pd.cut(y_test, bins=bins, labels=labels)\n",
    "\n",
    "    for label in labels:\n",
    "        mask = y_test_bins == label\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        mae_bin = mean_absolute_error(y_test[mask], np.array(y_pred)[mask])\n",
    "        rmse_bin = np.sqrt(mean_squared_error(y_test[mask], np.array(y_pred)[mask]))\n",
    "        print(f\"\\nRange RUL {label}:\")\n",
    "        print(f\"  MAE: {mae_bin:.2f}\")\n",
    "        print(f\"  RMSE: {rmse_bin:.2f}\")\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "    plt.xlabel('True RUL')\n",
    "    plt.ylabel('Predicted RUL')\n",
    "    plt.title('Predicted vs True RUL')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', color='red')\n",
    "    plt.show()\n",
    "    # Visualize errors for RUL <= 60\n",
    "    mask_short = y_test <= 60\n",
    "    errors = y_test[mask_short] - y_pred[mask_short]\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.hist(errors, bins=40, alpha=0.7)\n",
    "    plt.title('(True RUL - Predicted RUL) for RUL ≤ 60')\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim(-60, 30)\n",
    "    # Calculate errors and visualize confidence intervals \n",
    "    import scipy.stats as st\n",
    "    mean_err = np.mean(errors)\n",
    "    sem = st.sem(errors)\n",
    "    plt.axvline(mean_err, color='red', linestyle='--', label='Mean error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Weighted MAE\n",
    "    # We consider errors for RUL <= 60 three times more important\n",
    "    weights = np.where(y_test <= 60, 3, 1)  \n",
    "    weighted_mae = np.sum(weights * np.abs(y_test - y_pred)) / np.sum(weights)\n",
    "    print(f\"weighted MAE: {weighted_mae:.2f}\")\n",
    "\n",
    "def plot_feature_importances(model, X_train):\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # top 20 most important features\n",
    "    indices = np.argsort(importances)[::-1][:20]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(\"Feature importances (XGBoost)\")\n",
    "    plt.bar(range(len(indices)), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f24df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred_xgb)\n",
    "plot_feature_importances(model_xgb, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbed550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def plot_shap_values(model, X_test):\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    # Summary plot\n",
    "    shap.summary_plot(shap_values, X_test, max_display=20)\n",
    "\n",
    "\n",
    "plot_shap_values(model_xgb, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485a53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34523283",
   "metadata": {},
   "source": [
    "# 7. Baseline Catboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model_cat = CatBoostRegressor(early_stopping_rounds=50, iterations=1000, learning_rate=0.1, depth=5, random_seed=42, verbose=100)\n",
    "\n",
    "model_cat.fit(X_train, y_train)\n",
    "y_pred_cat = model_cat.predict(X_test)\n",
    "\n",
    "print(\"\\nCatBoost:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted RUL: {y_pred_cat[i]:.1f}, True RUL: {y_test.iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bd6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred_cat)\n",
    "plot_feature_importances(model_cat, X_train)\n",
    "plot_shap_values(model_cat, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e778d8",
   "metadata": {},
   "source": [
    "Our graph shows the following:  \n",
    "  \n",
    "The average error (red dotted line) is negative, i.e. the model overestimates the remaining service life on average (predicts more than it actually is).  \n",
    "  \n",
    "Distribution of errors - most errors are in the range from -10 to 10, but there is a long left tail (errors up to -50), i.e. sometimes the model is very wrong in the direction of overestimating the service life.  \n",
    "  \n",
    "Practical conclusion:  \n",
    "The model tends to be \"optimistic\" - it often believes that the engine will last longer than it actually does. This is dangerous for operation, since you may not have time to replace or service the engine in time.\n",
    "Recommendation:  \n",
    "It is worth refining the model or adding a penalty for overestimating the service life to reduce the negative bias of the error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50804aaf",
   "metadata": {},
   "source": [
    "# 8. Let's add a penalty for resource overvaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_asymmetric_loss(y_true, y_pred):\n",
    "    # y_true и y_pred — numpy массивы\n",
    "    residual = y_true - y_pred\n",
    "    grad = np.where(residual < 0, -2, -1) * np.sign(residual)  # 5 — a penalty for underestimation \n",
    "    hess = np.ones_like(residual)\n",
    "    return grad, hess\n",
    "\n",
    "model_xgb_penalty = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective=custom_asymmetric_loss\n",
    ")\n",
    "model_xgb_penalty.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred_xgb)\n",
    "plot_feature_importances(model_xgb_penalty, X_train)\n",
    "plot_shap_values(model_xgb_penalty, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa3e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
